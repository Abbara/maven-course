import os
from typing import Literal, Annotated, TypedDict
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, START, StateGraph, MessagesState
from langgraph.prebuilt import ToolNode
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import Tool
from langchain_core.output_parsers import StrOutputParser
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_experimental.tools import PythonREPLTool
from dotenv import load_dotenv


# Load environment variables from .env file
load_dotenv()

# Initialize tools
tavily_tool = TavilySearchResults(max_results=2)

# Define structured outputs for our agents
class ResearcherResponse(BaseModel):
    """
    Structured output for the Researcher agent, containing financial projections
    for the weight loss drug proposal.
    """
    Net_Profit_year_1: float = Field(description="Forecasted net profit to Bausch Health after sales of weight loss drug in year 1")
    Net_Profit_year_2: float = Field(description="Forecasted net profit to Bausch Health after sales of weight loss drug in year 2")
    Net_Profit_year_3: float = Field(description="Forecasted net profit to Bausch Health after sales of weight loss drug in year 3")
    Cost_of_Investment_year_1: float = Field(description="Forecasted cost of investment to Bausch Health for the weight loss drug in year 1")
    Cost_of_Investment_year_2: float = Field(description="Forecasted cost of investment to Bausch Health for the weight loss drug in year 2")
    Cost_of_Investment_year_3: float = Field(description="Forecasted cost of investment to Bausch Health for the weight loss drug in year 3")

class CritiqueResponse(BaseModel):
    """
    Structured output for the Critique agent, containing feedback on the proposal
    and a decision on whether to accept it.
    """
    proposal_feedback: str = Field(description="Feedback of the critique agent on the sales proposal generated for the weight loss drug by the proposal agent")
    accept: bool = Field(description="Whether to accept the proposal or not")

class AgentState(MessagesState):
    """
    Defines the state of our agent system, including step count, research response,
    proposal content, acceptance status, and generated graph code.
    """
    step_count: int
    researcher_response: ResearcherResponse
    proposal: str
    proposal_accepted: bool
    graph_code: str

# Define agent functions

def call_research_model(state: AgentState, config: RunnableConfig):
    """
    Function for the Research agent. It uses the ChatOpenAI model to generate
    responses based on the current state and user query.
    """
    system_prompt = SystemMessage("You are a helpful AI assistant, please respond to the user's query to the best of your ability!")
    model = ChatOpenAI(model="gpt-4o-mini")
    tools = [tavily_tool]
    model = model.bind_tools(tools)
    response = model.invoke([system_prompt] + state['messages'], config)
    current_count = state.setdefault('step_count', 0)
    return {"messages": [response], 'step_count': current_count + 1}

def respond(state: AgentState):
    """
    Function to generate a structured response using the ResearcherResponse model.
    This formulates the proposal based on the research conducted.
    """
    model = ChatOpenAI(model="gpt-4o-mini")
    response = model.with_structured_output(ResearcherResponse).invoke([HumanMessage(content=state['messages'][-1].content)])
    return {'proposal': state['messages'][-1].content, "researcher_response": response}

def call_critique_model(state: AgentState, config: RunnableConfig):
    """
    Function for the Critique agent. It reviews the proposal generated by the Research agent
    and provides feedback, including whether to accept the proposal or not.
    """
    critique_prompt = """You are tasked with reviewing a sales proposal generated by a LLM. Review the proposal and provide
    clear feedback and actionable feedback on any spelling mistakes, don't comment on any other aspect.
    If it seems like the proposal is fine, then in proposal feedback mention that the proposal is good and accept the proposal.
    Otherwise provide concise proposal feedback and reject the proposal.
    """
    model = ChatOpenAI(model="gpt-4o-mini")
    messages = [
        {"role": "user", "content": critique_prompt},
        {"role": "assistant", "content": state['proposal']},
    ]
    response = model.with_structured_output(CritiqueResponse).invoke(messages)
    accepted = response.accept
    if accepted:
        return {
            "messages": [
                {"role": "user", "content": response.proposal_feedback},
                {"role": "assistant", "content": "okay, sending this to coder agent"}],
            "proposal_accepted": True
        }
    else:
        return {
            "messages": [
                {"role": "user", "content": response.proposal_feedback},
            ],
            "proposal_accepted": False
        }

def route_critique(state: AgentState) -> Literal["research_agent", 'coder']:
    """
    Function to determine the next step after the critique. If the proposal is accepted
    or we've reached the maximum number of iterations, we move to the coder agent.
    Otherwise, we return to the research agent for refinement.
    """
    if state['proposal_accepted'] or state['step_count'] > 3:
        return 'coder'
    else:
        return "research_agent"

def call_coder_model(state: AgentState, config: RunnableConfig):
    """
    Function for the Coder agent. It generates Python code to create a graph
    representing the forecasted ROI based on the proposal data.
    """
    coder_prompt = """You are tasked creating a graph from a sales proposal generated by a LLM.
    Using the proposal write a clear, executable python code for creating a graph representing the forecasted ROI for next 3 years.
    The output should be executable python code only.
    """
    coder_agent_context = f"""
    proposal: {state['proposal']},
    Net_Profit_year_1: {state['researcher_response'].Net_Profit_year_1},
    Net_Profit_year_2: {state['researcher_response'].Net_Profit_year_2},
    Net_Profit_year_3: {state['researcher_response'].Net_Profit_year_3},
    Cost_of_Investment_year_1: {state['researcher_response'].Cost_of_Investment_year_1},
    Cost_of_Investment_year_2: {state['researcher_response'].Cost_of_Investment_year_2},
    Cost_of_Investment_year_3: {state['researcher_response'].Cost_of_Investment_year_3},
    """
    messages = [
        {"role": "user", "content": coder_prompt},
        {"role": "assistant", "content": coder_agent_context},
    ]
    model = ChatOpenAI(model="gpt-4o-mini")
    response = model.invoke(messages, config)
    return {"graph_code": response.content}

def should_continue(state: AgentState):
    """
    Function to determine whether to continue the research process or move to the response phase.
    """
    messages = state["messages"]
    last_message = messages[-1]
    if not last_message.tool_calls:
        return "respond"
    else:
        return "continue"

# Create the workflow
def create_workflow():
    """
    Function to create and configure the workflow graph for our multi-agent system.
    """
    search_tool_node = ToolNode([tavily_tool])
    workflow = StateGraph(AgentState)
    workflow.add_node("research_agent", call_research_model)
    workflow.add_node("respond", respond)
    workflow.add_node("search_tool", search_tool_node)
    workflow.add_node('critique', call_critique_model)
    workflow.add_node('coder', call_coder_model)
    workflow.add_edge(START, "research_agent")
    workflow.add_conditional_edges(
        "research_agent",
        should_continue,
        {
            "continue": "search_tool",
            "respond": "respond",
        },
    )
    workflow.add_edge("search_tool", 'research_agent')
    workflow.add_edge("respond", 'critique')
    workflow.add_conditional_edges("critique", route_critique)
    workflow.add_edge("coder", END)
    return workflow

# Initialize and compile the graph
checkpointer = MemorySaver()
graph = create_workflow().compile(checkpointer=checkpointer, interrupt_before=["coder"])
